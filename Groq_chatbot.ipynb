{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install groq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AD4b7WPj0k0G",
        "outputId": "b45476c4-4dca-4e67-abe5-46003a4e7b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.11.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq) (2.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq) (2.23.3)\n",
            "Downloading groq-0.11.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, groq\n",
            "Successfully installed groq-0.11.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "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"
      ],
      "metadata": {
        "id": "wcU15AA2W86b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a GROQ API Key - [GROQ Cloud](https://console.groq.com/playground)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uqQfNH1n1U5K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_p_1Vsxzq9p",
        "outputId": "16d9343f-ae27-4dd6-d80b-a8d742d77b44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fast language models, also known as efficient or low-latency language models, have become increasingly important in recent years due to their ability to process and generate human-like text quickly and efficiently. Here are some reasons why fast language models are important:\n",
            "\n",
            "1. **Real-time applications**: Fast language models are essential for real-time applications where instantaneous responses are crucial, such as:\n",
            "\t* Chatbots and virtual assistants\n",
            "\t* Sentiment analysis and text classification\n",
            "\t* Language translation and machine translation\n",
            "\t* Adversarial systems, such as chatbots that mimic human conversations\n",
            "2. **Large-scale data processing**: As the amount of text data continues to grow, fast language models can process and analyze large datasets quickly, making them ideal for tasks like:\n",
            "\t* Natural Language Processing (NLP) pipelines\n",
            "\t* Text classification and clustering\n",
            "\t* Sentiment analysis and opinion mining\n",
            "3. **Latency-sensitive applications**: Fast language models are necessary for applications that require fast response times, such as:\n",
            "\t* Online search and recommendation systems\n",
            "\t* Customer service and support\n",
            "\t* Social media and online communities\n",
            "4. **Hardware limitations**: As AI models grow in complexity and size, they need to be optimized for hardware limitations, such as:\n",
            "\t* Memory and processing power constraints\n",
            "\t* Limited battery life for edge devices\n",
            "\t* Energy efficiency for data centers and cloud computing\n",
            "5. **Improved user experience**: Fast language models can provide a better user experience by:\n",
            "\t* Reducing processing times for queries\n",
            "\t* Enabling faster feedback loops\n",
            "\t* Improving responsiveness and interaction\n",
            "6. **Competitive advantage**: Companies that can develop and deploy fast language models can gain a competitive advantage in the market, as they can:\n",
            "\t* Respond quickly to customer inquiries\n",
            "\t* Analyze data more efficiently\n",
            "\t* Provide more personalized and targeted services\n",
            "7. **Scientific and research applications**: Fast language models can also accelerate scientific discovery and research in areas like:\n",
            "\t* Linguistics and cognitive science\n",
            "\t* Computational linguistics and NLP\n",
            "\t* Data analysis and visualization\n",
            "\n",
            "Overall, fast language models have the potential to transform the way we interact with technology, enabling faster, more efficient, and more human-like conversations. As the demand for real-time and large-scale language processing continues to grow, the development of fast language models will play a crucial role in shaping the future of AI, NLP, and many other fields.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "from groq import Groq\n",
        "\n",
        "from google.colab import userdata\n",
        "# userdata.get('secretName')\n",
        "\n",
        "client = Groq(\n",
        "    # This is the default and can be omitted\n",
        "    api_key=userdata.get(\"GROQ_API_KEY\"),\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"you are a helpful assistant.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Explain the importance of fast language models\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama3-8b-8192\",\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PLRlXlkFCxjReAnxFl7Djpq4iDtv5ohZ"
      ],
      "metadata": {
        "id": "7s9ohkI80hJf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}